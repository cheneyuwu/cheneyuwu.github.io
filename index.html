<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

    <title>Yuchen Wu</title>

    <meta name="author" content="Yuchen Wu" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <link rel="stylesheet" type="text/css" href="stylesheet.css" />
    <link rel="icon" href="data/picture.jpeg" />
  </head>

  <body>
    <table
      style="
        width: 100%;
        max-width: 800px;
        border: 0px;
        border-spacing: 0px;
        border-collapse: separate;
        margin-right: auto;
        margin-left: auto;
      "
    >
      <tbody>
        <tr style="padding: 0px">
          <td style="padding: 0px">
            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr style="padding: 0px">
                  <td style="padding: 2.5%; width: 63%; vertical-align: middle">
                    <p style="text-align: center">
                      <name>Yuchen Wu</name>
                    </p>
                    <p>
                      I am a MASc student in Robotics at
                      <a href="https://www.utoronto.ca/">University of Toronto</a> supervised by
                      <a href="http://asrl.utias.utoronto.ca/~tdb/">Prof. Tim Barfoot</a>. I am a part of the
                      <a href="http://asrl.utias.utoronto.ca/">Autonomous Space Robotics Laboratory (ASRL)</a> and the
                      <a href="https://robotics.utoronto.ca/">UofT Robotics Institute</a>.
                    </p>
                    <p>
                      I received my BASc degree from <a href="https://engsci.utoronto.ca/">UofT Engineering Science</a> (Robotics). During my undergraduate study, I worked with <a href="http://www.cs.toronto.edu/~florian/">Prof. Florian Shkurti</a> at the <a href="https://rvl.cs.toronto.edu/">Robot Vision & Learning</a> lab on imitation and reinforcement learning.
                    </p>
                    <p style="text-align: center">
                      <a href="mailto:cheneyuwu@gmail.com">Email</a> &nbsp/&nbsp
                      <a href="data/cv/cv.pdf">CV</a> &nbsp/&nbsp
                      <a href="https://www.linkedin.com/in/yuchen-wu-a9838a14a">LinkedIn</a> &nbsp/&nbsp
                      <a href="https://scholar.google.com/citations?user=Niv8kqsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                      <a href="https://github.com/cheneyuwu/">GitHub</a>
                    </p>
                  </td>
                  <td style="padding: 2.5%; width: 40%; max-width: 40%">
                    <a href="data/picture.jpeg"
                      ><img
                        style="width: 100%; max-width: 100%; border-radius: 50%"
                        alt="profile photo"
                        src="data/picture.jpeg"
                        class="hoverZoomLink"
                    /></a>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 20px; width: 100%; vertical-align: middle">
                    <heading>Research</heading>
                    <p>
                      I'm interested in mobile robot state estimation. My research currently focuses on lidar & radar mapping and localization.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 20px; width: 35%; vertical-align: middle">
                    <iframe width="100%" src="https://www.youtube.com/embed/okS7pF6xX7A" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  </td>
                  <td width="65%" valign="middle">
                    <a href="https://arxiv.org/abs/2203.10174">
                      <papertitle>
                        Are We Ready for Radar to Replace Lidar in All-Weather Mapping and Localization?
                      </papertitle>
                    </a>
                    <br />
                    <a href="http://asrl.utias.utoronto.ca/~keenan/">Keenan Burnett*</a>,
                    <strong>Yuchen Wu*</strong>,
                    <a href="https://scholar.google.ca/citations?user=uoH44gEAAAAJ&hl=en">David J. Yoon</a>,
                    <a href="https://www.dynsyslab.org/prof-angela-schoellig/">Angela P. Schoellig</a>,
                    <a href="http://asrl.utias.utoronto.ca/~tdb/">Timothy D. Barfoot</a>
                    <br />
                    Accepted to <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2022
                    <br />
                    <a href="https://arxiv.org/pdf/2203.10174.pdf">paper</a> /
                    <a href="https://www.youtube.com/watch?v=okS7pF6xX7A&list=PLC0E5EB919968E507">video</a> /
                    <a href="https://github.com/utiasASRL/vtr3">code</a> /
                    <a href="data/bib/burnett_iros22.bib">bibtex</a>
                    <p style="margin-top:5pt;">
                      We present an extensive comparison between three topometric localization systems: radar-only, lidar-only, and a cross-modal radar-to-lidar system across varying seasonal and weather conditions using the Boreas dataset.
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding: 20px; width: 35%; vertical-align: middle">
                    <iframe width="100%" src="https://www.youtube.com/embed/Cay6rSzeo1E" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  </td>
                  <td width="65%" valign="middle">
                    <a href="https://www.boreas.utias.utoronto.ca/#/">
                      <papertitle>Boreas: A Multi-Season Autonomous Driving Dataset </papertitle>
                    </a>
                    <br />
                    <a href="http://asrl.utias.utoronto.ca/~keenan/">Keenan Burnett</a>,
                    <a href="https://scholar.google.ca/citations?user=uoH44gEAAAAJ&hl=en">David J. Yoon</a>,
                    <strong>Yuchen Wu</strong>,
                    Andrew Zou Li,
                    Haowei Zhang,
                    Shichen Lu,
                    Jingxing Qian,
                    Wei-Kang Tseng,
                    Andrew Lambert,
                    Keith Y.K. Leung,
                    <a href="https://www.dynsyslab.org/prof-angela-schoellig/">Angela P. Schoellig</a>,
                    <a href="http://asrl.utias.utoronto.ca/~tdb/">Timothy D. Barfoot</a>
                    <br />
                    Submitted to <em>the International Journal of Robotics Research (IJRR)</em>
                    <br />
                    <a href="https://www.boreas.utias.utoronto.ca/#/">website</a> /
                    <a href="https://arxiv.org/pdf/2203.10168.pdf">paper</a> /
                    <a href="https://www.youtube.com/watch?v=Cay6rSzeo1E">video</a> /
                    <a href="https://github.com/utiasASRL/pyboreas">code</a> /
                    <a href="data/bib/burnett_ijrr.bib">bibtex</a>
                    <p style="margin-top:5pt;">
                      The Boreas dataset was collected by driving a repeated route over the course of 1 year resulting in stark seasonal variations. In total, Boreas contains over 350km of driving data including several sequences with adverse weather conditions such as rain and heavy snow.
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding: 20px; width: 35%; vertical-align: middle">
                    <iframe width="100%" src="https://www.youtube.com/embed/KkG6TQOVXak" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  </td>
                  <td width="65%" valign="middle">
                    <a href="https://3dv2021.surrey.ac.uk/demos/">
                      <papertitle>
                        Visual Teach & Repeat using Deep Learned Features
                      </papertitle>
                    </a>
                    <br />
                    <a href="http://asrl.utias.utoronto.ca/~keenan/">Mona Gridseth</a>,
                    <strong>Yuchen Wu</strong>,
                    <a href="http://asrl.utias.utoronto.ca/~tdb/">Timothy D. Barfoot</a>
                    <br />
                    Demo at <em>International Conference on 3D Vision (3DV)</em>, 2021
                    <br />
                    <a href="https://www.youtube.com/watch?v=okS7pF6xX7A&list=PLC0E5EB919968E507">video</a> /
                    <a href="https://github.com/utiasASRL/vtr3">code</a>
                    <p style="margin-top:5pt;">
                      We provide a demo of Visual Teach and Repeat 3 for autonomous path following on a mobile robot, which uses deep learned features to tackle localization across challenging appearance change. Corresponding paper on deep learned features: <a href="https://arxiv.org/abs/2109.04041">link</a>.
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding: 20px; width: 35%; vertical-align: middle">
                    <iframe width="100%" src="https://www.youtube.com/embed/E5Tg9juP8ck" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  </td>
                  <td width="65%" valign="middle">
                    <a href="https://github.com/utiasASRL/vtr3">
                      <papertitle>Visual Teach & Repeat 3</papertitle>
                    </a>
                    <br />
                    <strong>Yuchen Wu</strong>,
                    <a href="linkedin.com/in/ben-congram">Ben Congram</a>,
                    <a href="linkedin.com/in/zi-cong-daniel-guo">Daniel Guo</a>
                    <br />
                    Open Source Project
                    <br />
                    <a href="https://utiasasrl.github.io/vtr3/">website</a> /
                    <a href="https://youtu.be/E5Tg9juP8ck">video</a> /
                    <a href="https://github.com/utiasASRL/vtr3">code</a>
                    <p style="margin-top:5pt;">
                      VT&R3 is a C++ implementation of the Teach and Repeat navigation framework developed at <a href="http://asrl.utias.utoronto.ca/">ASRL</a>. It allows user to teach a robot a large (kilometer-scale) network of paths where the robot navigate freely via accurate (centimeter-level) path following, using a lidar/radar/camera as the primary sensor (no GPS).
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding: 20px; width: 35%; vertical-align: middle">
                    <iframe width="100%" src="https://www.youtube.com/embed/rH56GpbTTnw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  </td>
                  <td width="65%" valign="middle">
                    <a href="https://ieeexplore.ieee.org/document/9561333">
                      <papertitle>
                        Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models
                      </papertitle>
                    </a>
                    <br />
                    <strong>Yuchen Wu</strong>,
                    <a href="https://mila.quebec/en/person/melissa-mozifian/">Malissa Mozifian</a>
                    <a href="http://www.cs.toronto.edu/~florian/">Florian Shkurti</a>
                    <br />
                    <em>International Conference on Robotics and Automation (ICRA)</em>, 2021
                    <br />
                    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9561333">paper</a> /
                    <a href="data/bib/wu_icra21.bib">bibtex</a>
                    <p style="margin-top:5pt;">
                      We propose a method that combines reinforcement and imitation learning by shaping the reward function with a state-and-action-dependent potential that is trained from demonstration data, using a generative model.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 0px">
                    <p style="text-align: right; font-size: small">
                      <a href="http://jonbarron.info">Website Template</a>
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
